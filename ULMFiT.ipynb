{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning a forward and backward langauge model to get to 95.4% accuracy on the IMDB movie reviews dataset. This tutorial is done with fastai v1.0.53."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modified mixup callback and forward functions for AWD_LSTM\n",
    "from MixUp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From a language model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was run on a Titan RTX (24 GB of RAM) so you will probably need to adjust the batch size accordinly. If you divide it by 2, don't forget to divide the learning rate by 2 as well in the following cells. You can also reduce a little bit the bptt to gain a bit of memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt=64,60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will download and untar the file containing the IMDB dataset, returning a `Pathlib` object pointing to the directory it's in (default is ~/.fastai/data/imdb0). You can specify another folder with the `dest` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then gather the data we will use to fine-tune the language model using the [data block API](https://docs.fast.ai/data_block.html). For this step, we want all the texts available (even the ones that don't have lables in the unsup folder) and we won't use the IMDB validation set (we will do this for the classification part later only). Instead, we set aside a random 10% of all the texts to build our validation set.\n",
    "\n",
    "The fastai library will automatically launch the tokenization process with the [spacy tokenizer](https://spacy.io/api/tokenizer/) and a few [default rules](https://docs.fast.ai/text.transform.html#Rules) for pre and post-processing before numericalizing the tokens, with a vocab of maximum size 60,000. Tokens are sorted by their frequency and only the 60,000 most commom are kept, the other ones being replace by an unkown token. This cell takes a few minutes to run, so we save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = (TextList.from_folder(path)\n",
    "           #Inputs: all the text files in path\n",
    "            .filter_by_folder(include=['train', 'test', 'unsup']) \n",
    "           #We may have other temp folders that contain text files so we only keep what's in train, test and unsup\n",
    "            .split_by_rand_pct(0.1)\n",
    "           #We randomly split and keep 10% (10,000 reviews) for validation\n",
    "            .label_for_lm()           \n",
    "           #We want to do a language model so we label accordingly\n",
    "            .databunch(bs=bs, bptt=bptt))\n",
    "data_lm.save('data_lm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When restarting the notebook, as long as the previous cell was executed once, you can skip it and directly load your data again with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = load_data(path, 'data_lm.pkl', bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are training a language model, all the texts are concatenated together (with a random shuffle between them at each new epoch). The model is trained to guess what the next word in the sentence is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>it disguised the fact that their \" monsters \" were so stupid - looking . i also do n't think it 's a coincidence that the writer is xxmaj sid \" xxmaj pink \" . \\n \\n  xxmaj this movie is good for a laugh , if you are really looking for a movie made in 9 days on 200,000 dollars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\\n \\n  xxmaj when someone has a caricature done of themselves , they do n't go home and say , \" xxmaj hey , let 's make the ears and nose even bigger ! \" xxmaj that 's what xxmaj lee has done in this film . xxmaj the most interesting characters in the film are the two ( xxmaj adrian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>the only battle scene of the film . xxmaj then you see xxmaj daniel xxmaj xxunk ( who was xxmaj xxunk in xxmaj city of xxmaj lost xxmaj children ) for about two seconds , and that would let anyone hope the film will have good acting . xxmaj unfortunately he is very bad in the film . xxmaj the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>has been destroyed centuries ago reappears for no reason just to collapse on top of her . i mean , that makes no sense . xxmaj what the hell was xxmaj charles thinking allowing this pile of puke to be made , with four different movie companies they were that desperate for movies . xxmaj they could have asked me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>a ' tough chick ' going around kicking ass ... when why does n't she do very much of it ? \" \\n \\n  \" i ca n't believe how unoriginal this dialogue is . \" \\n \\n  \" xxmaj how long is this thing ? i feel like i 've been watching it for over four hours already . \" \\n \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a backward model, the only difference is we'll have to pqss the flag `backwards=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bwd = load_data(path, 'data_lm.pkl', bs=bs, bptt=bptt, backwards=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>xxmaj : comment final my xxmaj \\n \\n  . down xxup song the of pleasure overall the drag they , filming shoddy and angles camera the with but , movie the throughout songs two hear 'll you xxmaj : fans filth xxmaj of xxmaj cradle xxmaj you to xxmaj \\n \\n  . shirt 's man the in blood fake with filled packet juice a 's there like looks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>character the for nothing felt i . thing whole the during pain in was and movie the watched i . craze movie action big the during set was it xxmaj ? trouble the xxmaj . more much so xxup been have could movie this xxmaj . family his and john xxmaj character main the for felt i and bachman xxmaj as books kings xxmaj from man running the read i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>characters these of sense make to try to harder even it made mother the by comment \" today died father your \" explained never the as such screenplay the in holes enormous xxmaj . viewers most of ask to much too was out played structure scene the way the in nature long painfully the that thought certainly most i , intrigue 's it of way by film this in qualities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>all in all but , long too development character the , standards 's today by laughable were effects special the of quality the xxmaj . movies of types these of expect to come 've i what was out xxmaj watch xxmaj better xxmaj you xxmaj . xxunk xxmaj on these of some see to excited very was i so , disappointed very was i , theaters our from pulled was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>perhaps xxmaj . will free own their ... by safe the to returned are hobgoblins xxmaj the xxmaj . \" ? what xxmaj \" wondering watcher the leave will movie this of end the at twist the xxmaj : ahead xxup spoilers xxup warning xxup \\n \\n  . budget big a having film the or ... dollars of millions having with do to anything has never which fantasy wildest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_bwd.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the forward language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind the [ULMFit paper](https://arxiv.org/abs/1801.06146) is to use transfer learning for this classification task. Our language model isn't randomly initialized but with the weights of a model pretrained on a larger corpus, [Wikitext 103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/). The vocabulary of the two datasets are slightly different, so when loading the weights, we take care to put the embedding weights at the right place, and we rando;ly initiliaze the embeddings for words in the IMDB vocabulary that weren't in the wikitext-103 vocabulary of our pretrained model.\n",
    "\n",
    "This is all done by the first line of code that will download the pretrained model for you at the first use. The second line is to use [Mixed Precision Training](), which enables us to use a higher batch size by training part of our model in FP16 precision, and also speeds up training by a factor 2 to 3 on modern GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, AWD_LSTM)\n",
    "#learn = learn.to_fp16(clip=0.1) My GPU doesn't really benefit from Mixed Precision Training so I comment this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2e-3/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Learner` object we get is frozen by default, which means we only train the embeddings at first (since some of them are random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.434217</td>\n",
       "      <td>4.125229</td>\n",
       "      <td>0.288479</td>\n",
       "      <td>29:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 5e-3, moms=(0.8,0.7), wd=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we unfreeze the model and fine-tune the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.153997</td>\n",
       "      <td>3.948784</td>\n",
       "      <td>0.308919</td>\n",
       "      <td>33:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.065418</td>\n",
       "      <td>3.880159</td>\n",
       "      <td>0.317173</td>\n",
       "      <td>33:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.997172</td>\n",
       "      <td>3.829845</td>\n",
       "      <td>0.323110</td>\n",
       "      <td>33:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.954616</td>\n",
       "      <td>3.811984</td>\n",
       "      <td>0.325213</td>\n",
       "      <td>33:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(4, 5e-4, moms=(0.8,0.7), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.009093</td>\n",
       "      <td>3.859601</td>\n",
       "      <td>0.319627</td>\n",
       "      <td>33:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.973456</td>\n",
       "      <td>3.818596</td>\n",
       "      <td>0.324950</td>\n",
       "      <td>33:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.926792</td>\n",
       "      <td>3.788066</td>\n",
       "      <td>0.328386</td>\n",
       "      <td>33:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(3, 5e-4, moms=(0.8,0.7), wd=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done, we jsut save the encoder of the model (everything except the last linear layer that was decoding our final hidden states to words) because this is what we will use for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.915986</td>\n",
       "      <td>3.783882</td>\n",
       "      <td>0.328897</td>\n",
       "      <td>33:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-4, moms=(0.8,0.7), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('fwd_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same but backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't directly train a bidirectional RNN for language modeling, but you can always enseble a forward and backward model. fastai provides a pretrained forward and backawrd model, so we can repeat the previous step to fine-tune the pretrained backward model. The command `language_model_learner` checks the `data` object you pass to automatically decide if it should use the pretrained forward or backward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_bwd, AWD_LSTM)\n",
    "#learn = learn.to_fp16(clip=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the training is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.335882</td>\n",
       "      <td>4.022331</td>\n",
       "      <td>0.326608</td>\n",
       "      <td>06:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.062783</td>\n",
       "      <td>3.886092</td>\n",
       "      <td>0.342082</td>\n",
       "      <td>06:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.962229</td>\n",
       "      <td>3.810247</td>\n",
       "      <td>0.350727</td>\n",
       "      <td>06:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.922255</td>\n",
       "      <td>3.766445</td>\n",
       "      <td>0.355493</td>\n",
       "      <td>06:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.874089</td>\n",
       "      <td>3.730033</td>\n",
       "      <td>0.359623</td>\n",
       "      <td>06:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.837302</td>\n",
       "      <td>3.702043</td>\n",
       "      <td>0.362982</td>\n",
       "      <td>06:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.803697</td>\n",
       "      <td>3.675028</td>\n",
       "      <td>0.365694</td>\n",
       "      <td>06:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.754358</td>\n",
       "      <td>3.650825</td>\n",
       "      <td>0.368455</td>\n",
       "      <td>06:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.713858</td>\n",
       "      <td>3.629092</td>\n",
       "      <td>0.370696</td>\n",
       "      <td>06:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.676009</td>\n",
       "      <td>3.617974</td>\n",
       "      <td>0.371980</td>\n",
       "      <td>06:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.649889</td>\n",
       "      <td>3.615306</td>\n",
       "      <td>0.372268</td>\n",
       "      <td>06:48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, 2e-3, moms=(0.8,0.7), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('bwd_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... to a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier is a model that is a bit heavier, so we have lower the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the data block API again to gather all the texts for classification. This time, we only keep the ones in the trainind and validation folderm and label then by the folder they are in. Since this step takes a bit of time, we save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = (TextList.from_folder(path, vocab=data_lm.vocab)\n",
    "             #grab all the text files in path\n",
    "             .split_by_folder(valid='test')\n",
    "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "             .label_from_folder(classes=['neg', 'pos'])\n",
    "             #label them all with their folders\n",
    "             .databunch(bs=bs))\n",
    "\n",
    "data_clas.save('data_clas.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as the previous cell was executed once, you can skip it and directly do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = load_data(path, 'data_clas.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace Forward functions to allow MixUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWD_LSTM.forward = AWDforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiBatchEncoder.forward = MBEforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj by now you 've probably heard a bit about the new xxmaj disney dub of xxmaj miyazaki 's classic film , xxmaj laputa : xxmaj castle xxmaj in xxmaj the xxmaj sky . xxmaj during late summer of 1998 , xxmaj disney released \" xxmaj kiki 's xxmaj delivery xxmaj service \" on video which included a preview of the xxmaj laputa dub saying it was due out</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj some have praised _ xxunk _ as a xxmaj disney adventure for adults . i do n't think so -- at least not for thinking adults . \\n \\n  xxmaj this script suggests a beginning as a live - action movie , that struck someone as the type of crap you can not sell to adults anymore . xxmaj the \" crack staff \" of many older</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos * * * xxmaj warning - this review contains \" plot spoilers , \" though nothing could \" spoil \" this movie any more than it already is . xxmaj it really xxup is that bad . * * * \\n \\n  xxmaj before i begin , i 'd like to let everyone know that this definitely is one of those so - incredibly - bad - that</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the sweetest and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, you only have to add `backwards=True` to load the data for a backward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas_bwd = load_data(path, 'data_clas.pkl', bs=bs, backwards=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>\\n \\n  a- xxup a ppv xxup this give i . winner a was one this but , good very n't were ppv xxup the lately xxmaj ! ppv xxup decent a is there xxunk \\n \\n  rock xxmaj the xxmaj : champion xxmaj wwe xxup new xxmaj and winner xxmaj ! pinned and one xxmaj great xxmaj the by bottomed xxmaj rock xxmaj was but slam xxmaj</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9 - score xxmaj a - grade xxmaj \\n \\n  ... better is think you what on decision a make can you xxmaj . realistic hardly but inspiring is that legend a about film a is \" braveheart xxmaj \" . world 's today in apropos very are that themes has that film a is \" roy xxmaj rob xxmaj \" , simply put to xxmaj . \" roy</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\\n \\n  . you for is definitely movie this then , disaster face they how and 's 1900 early the in characters of life the into insight an you give will that characters interesting like you if xxmaj . editing and , xxunk , sound , costuming including material award xxmaj academy xxmaj are film this of aspects certain xxmaj . picture this in found be can that treats</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>. lower ever seeps it , plot the about think i as but enjoyable is movie -the - 10 of out 4 \\n \\n  . badness much so avoided and feel pulp its of none lost ark xxmaj lost xxmaj the of raiders xxmaj . well as up grow to needs pulp our and sophisticated more bit a become have we xxmaj . anyway , \" anymore adults sell</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>. wrestlers of roster great a handle to way poor a is which hogan xxmaj and warrior xxmaj by decimated were heels the of most xxmaj . pinned be to belt conveyor a on waiting almost were wrestlers the as effect detrimental a had obviously time little too and matches many too , overall xxmaj \\n \\n  10 / 2 . nauseous of point the to ending predictable very</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas_bwd.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the forward classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier needs a little less dropout, so we pass `drop_mult=0.5` to multiply all the dropouts by this amount (it's easier than adjusting all the five different values manually). We don't load the pretrained model, but instead our fine-tuned encoder from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj this movie is funny and painful at the same time . xxmaj the \" xxmaj cinemagic \" almost gave me a seizure . xxmaj despite what they imply , \" xxmaj cinemagic \" is not some innovative technical procedure . xxmaj it was \" developed \" as the result of an accident , and they used it because it disguised the fact that their \" monsters \" were so stupid - looking . i also do n't think it 's a coincidence that the writer is xxmaj sid \" xxmaj pink \" . \n",
       " \n",
       "  xxmaj this movie is good for a laugh , if you are really looking for a movie made in 9 days on 200,000 dollars . xxmaj it is entertaining ; at least i can say that about it . xxmaj the bat / rat / spider is the highlight .,xxbos xxup xxunk , a.k.a . xxup alien xxup visitor , is not what i expected . xxmaj this is a no - budget xxmaj australian film with no special effects other than speeded - up film and quick scene cuts . xxmaj the female alien ( who comes over immediately able to speak perfectly accented xxmaj australian ) can \" blip \" from place to place or time to time and alter her perception of the flow of time to match the \" faster \" humans . \n",
       " \n",
       "  xxmaj an elderly grandmother tells her two granddaughters about a story a wandering man told her 40 years before , when an unnamed \" xxmaj she \" came to the planet naked and completely disoriented , unable to recognize which star in the sky she came from ... xxmaj she meets a man alone camping in the xxmaj australian xxmaj outback , apparently bewildering him . xxmaj she is here by \" mistake \" , and gets angry when she is told she is on xxmaj earth . xxmaj the xxmaj earthlings are known as consummate xxunk of the environment and a metaphor for the most insulting thing imaginable to the rest of the universe : those who \" breathe the foul air \" but do nothing about it , sticking their heads under the sand like an ostrich . xxmaj in another amusing metaphor , xxmaj earthlings are \" frogs \" . \n",
       " \n",
       "  xxmaj from there , it is entirely a film about dialogue , as the perplexed man tries to understand xxmaj she 's peculiar psychology and viewpoints , even as xxmaj she calls him unintelligent and \" quaint \" . xxmaj the man begins to realize maybe it 's humans who are irrational and not thinking straight . xxmaj yet , while waiting to be \" beamed up \" back home , xxmaj she sees that this human is not entirely faulty in his thinking and even falls in love with him . \n",
       " \n",
       "  xxmaj the dialogue about perspectives is in spots interesting , but it is all layered with a heavy - handed environmental message and a low - budget feel ( there are only two main actors , who blip around various deserted scenes , and the evil despoiling humans on the planet are never seen at all ) . xxmaj the environmental message offers no solutions , but paints one or two dire metaphors about what will happen to nature and man if something is n't done . xxmaj the logic also does n't hang together : the rest of the universe has \" given up \" on xxmaj earth , yet one space woman caught on xxmaj earth by mistake manages to effect some positive change by the conclusion of the movie . xxmaj what would a battalion of aliens deliberately sent here manage to achieve against pollution and waste ?,xxbos xxmaj after stopping by the movie store to find something to watch , we stumbled on this . xxmaj it looked appealing from the summary , at least , so we gave it a try . xxmaj and here 's the kicker : the first 20 minutes are interesting ! xxmaj it 's actually enjoyable ! xxmaj oh , wait , spoke too soon . \n",
       " \n",
       "  xxmaj somewhere in there , the movie took a disgusting turn into fundamental , right - wing xxmaj christian brain - washing . xxmaj not entirely sure what happens , but i think the screenplay writer found xxmaj god somewhere in there , finished writing this script , and had no time to edit it because he had a xxup kkk meeting to get to with his friends from the xxmaj xxunk xxmaj church and his hood was n't clean . \n",
       " \n",
       "  xxmaj can they put warnings on this ? i refuse to support this religious idiocy . xxmaj much like video games have rating systems , movies need some sort of symbol : maybe a small cross in the bottom corner to show us that a movie is going to take a turn for the worse . \n",
       " \n",
       "  xxmaj unless you share sentiments with whatever moron came up with this story , and will have your xxmaj bible open in your lap while you watch this and plan on how you 'll convert your neighbors , do n't waste your time . xxmaj it 's some of the worst junk that 's come out in a very long time , and the radical religious nuts do n't need anymore funding .,xxbos xxmaj the story line was very straight forward and easy to follow and contained a lot of no - brainer comedy to a point where it just got boring . xxmaj some of the audience seemed to find it funny but i like more intelligent humor . \n",
       " \n",
       "  xxmaj there were several known xxmaj swedish actors in the movie and their performance were decent considering the script . xxmaj lena xxmaj endre was good looking as always . \n",
       " \n",
       "  i do n't remember the original movie so i ca n't say if it 's better or worse . \n",
       " \n",
       "  xxmaj if you enjoy movies like xxmaj xxunk this movie might be worth taking a look at .,xxbos xxmaj no wonder this was released straight to xxup dvd here in xxmaj australia , no redeeming features what so ever . xxmaj the dialog was hokey , the acting , awful and the script sucked ! ! xxmaj whoever thought it would be a good idea to do a sequel or follow up to the far superior xxmaj john xxmaj badham film , xxmaj wargames from the 80s , well they must of been on something cause it was a bad idea ! ! xxmaj amanda xxmaj walsh was good in it as the eye candy / love interest , while xxmaj matt xxmaj xxunk was good as the other main lead- that is about it . i would not recommend xxmaj wargames : xxmaj the xxmaj dead xxmaj code to anyone , check out xxmaj hackers or the original xxmaj wargames film- both are better than this piece of crap ! !\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /storage/imdb;\n",
       "\n",
       "Valid: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj if you 're in the mood to laugh at a truly bad movie ( bad in the way only xxmaj ken xxmaj russell at his worst can be ) , you must try this one . xxmaj it succeeds in making you feel like you just landed in a xxunk porn - booth , and you can just about smell the urine on the floor . xxmaj kathleen xxmaj turner struts around in a blond wig , getting her kicks from \" pretending \" to be a two - bit hooker ( she really has a good solid job in the clothing industry and has been hurt so badly by men that this is the only way she can connect ) , and xxmaj tony xxmaj perkins plays a hysterical \" priest \" who is out to maybe murderer her ( yet another movie that ends with xxmaj tony xxmaj perkins in drag ) . xxmaj annie xxmaj potts shows up and is not allowed to provide an ounce of her usual wit , which is reason enough to hate this movie . xxmaj the kinky will love the sex scenes , so rent the unrated version in the xxup red box so you can see xxmaj turner give a cop a taste of his billy club ( i had to pause the xxup vcr until we stopped laughing ) .,xxbos i have n't seen this film in years , but the awful \" taste \" of xxmaj quaid 's performance still lingers on my tongue . xxmaj some have commented on how xxmaj quaid has xxmaj jerry xxmaj lee xxmaj lewis \" to a tee \" but the fact is he only appears to have the most extreme stage xxmaj jerry in mind . xxmaj nobody acts that way all the time , and the performance comes off as hopelessly clownish , reducing xxmaj lewis to a buffoonish caricature . xxmaj the nuances of a man 's life are lost in the rubble of sheer over - acting . \n",
       " \n",
       "  xxmaj the author of the book this is based on ( xxmaj nick xxmaj xxunk ) is a good writer , who has written several fine musical bios ( i particularly liked \" xxmaj dino \" on xxmaj dean xxmaj martin ) ; in the books xxmaj xxunk gives us a full human being , both separate from and involved in the \" biz . \" xxmaj quaid 's acting seems to imply that xxmaj jerry never acted like a human being . xxmaj if people were like this , no one would bother to hang around them . xxmaj as cartoons go , it is mildly amusing , but otherwise it is one of the most egregious , film - destroying performances i have had the \" honor \" of viewing . xxmaj terrible ...,xxbos \" xxmaj julia xxmaj xxunk ( xxmaj catherine xxmaj mary xxmaj stewart ) is working hard to become a doctor . xxmaj suddenly , xxmaj julia finds herself the guardian of her young niece , xxmaj amanda , after her parents are found murdered . xxmaj julia has a new neighbor , the mysterious xxmaj kevin xxmaj finney ( xxmaj rob xxmaj lowe ) . xxmaj her hectic life comes crashing down when it becomes apparent that the young xxmaj amanda holds the key to a terrible secret . xxmaj for she too is now the target of the killers . xxmaj julia must discover whether xxmaj kevin is friend or foe , and unlock the sinister mystery before the murderer strikes again , \" according to the xxup dvd sleeve synopsis . \n",
       " \n",
       "  xxmaj this thriller quietly falls apart at the seams , but it is interesting some of the time . xxmaj stalker xxmaj sean xxmaj devine 's background telephone scene ( outside the police station ) and xxmaj mr. xxmaj lowe 's violin ( restaurant ) are tensely played . xxmaj but , early on , it 's difficult not to miss the fact that one of the supposedly sophisticated killers purposely twists his foot in some red paint at the murder scene . xxmaj unfortunately , this is not an intricate plot point ; and , there are worse story stumbles afoot . xxmaj overall \" xxmaj dead xxmaj silent \" is not a bad way to spend some time , if there is nothing better on , or you 're into xxmaj lowe and his co - stars . \n",
       " \n",
       "  xxrep 4 * xxmaj dead xxmaj silent ( 1999 ) xxmaj roger xxmaj cardinal ~ xxmaj catherine xxmaj mary xxmaj stewart , xxmaj rob xxmaj lowe , xxmaj arlen xxmaj xxunk - xxmaj stewart , xxmaj larry xxmaj day,xxbos i watched about the first 30 - 40 minutes of this movie on television the other night and can agree that this is by far the worst of the series . xxmaj not any of it is funny , even xxmaj randy xxmaj quaid ca n't save this mess . xxmaj eric xxmaj idle was n't funny in xxup xxunk xxmaj euro . xxmaj vacation , and he 's even worse here . xxmaj the only funny scene is where they 're at the airport and some guy dressed as xxmaj santa walks by the camera yelling \" xxmaj did anybody lose this ? \" as he holds up a prosthetic leg ... \n",
       " \n",
       "  1 / 2 a star out of xxrep 4 *,xxbos xxmaj it looked cool from the movie sleeve , but after five minutes we were n't sure if it was a homosexual documentary of west side story without any female interest . xxmaj the film quality was poor , and there was hardly enough gang fighting action to sustain even the xxunk person 's interest for long enough to watch the entire film . xxmaj may god have mercy on the souls of both the actors and the filmmakers responsible for what i can only describe as my new one and only reason why i never will want to see ( or trust ) an xxmaj australian made film again . i have to write more so i will again say that the actors were so bad that i 'm positive i could make a better movie with fifteen dollars and a box of xxmaj xxunk . xxmaj please do n't see this movie for your own sake .\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /storage/imdb;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=LabelSmoothingCrossEntropy(), metrics=[<function accuracy at 0x7f5c1b29df28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/storage/imdb'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj this movie is funny and painful at the same time . xxmaj the \" xxmaj cinemagic \" almost gave me a seizure . xxmaj despite what they imply , \" xxmaj cinemagic \" is not some innovative technical procedure . xxmaj it was \" developed \" as the result of an accident , and they used it because it disguised the fact that their \" monsters \" were so stupid - looking . i also do n't think it 's a coincidence that the writer is xxmaj sid \" xxmaj pink \" . \n",
       " \n",
       "  xxmaj this movie is good for a laugh , if you are really looking for a movie made in 9 days on 200,000 dollars . xxmaj it is entertaining ; at least i can say that about it . xxmaj the bat / rat / spider is the highlight .,xxbos xxup xxunk , a.k.a . xxup alien xxup visitor , is not what i expected . xxmaj this is a no - budget xxmaj australian film with no special effects other than speeded - up film and quick scene cuts . xxmaj the female alien ( who comes over immediately able to speak perfectly accented xxmaj australian ) can \" blip \" from place to place or time to time and alter her perception of the flow of time to match the \" faster \" humans . \n",
       " \n",
       "  xxmaj an elderly grandmother tells her two granddaughters about a story a wandering man told her 40 years before , when an unnamed \" xxmaj she \" came to the planet naked and completely disoriented , unable to recognize which star in the sky she came from ... xxmaj she meets a man alone camping in the xxmaj australian xxmaj outback , apparently bewildering him . xxmaj she is here by \" mistake \" , and gets angry when she is told she is on xxmaj earth . xxmaj the xxmaj earthlings are known as consummate xxunk of the environment and a metaphor for the most insulting thing imaginable to the rest of the universe : those who \" breathe the foul air \" but do nothing about it , sticking their heads under the sand like an ostrich . xxmaj in another amusing metaphor , xxmaj earthlings are \" frogs \" . \n",
       " \n",
       "  xxmaj from there , it is entirely a film about dialogue , as the perplexed man tries to understand xxmaj she 's peculiar psychology and viewpoints , even as xxmaj she calls him unintelligent and \" quaint \" . xxmaj the man begins to realize maybe it 's humans who are irrational and not thinking straight . xxmaj yet , while waiting to be \" beamed up \" back home , xxmaj she sees that this human is not entirely faulty in his thinking and even falls in love with him . \n",
       " \n",
       "  xxmaj the dialogue about perspectives is in spots interesting , but it is all layered with a heavy - handed environmental message and a low - budget feel ( there are only two main actors , who blip around various deserted scenes , and the evil despoiling humans on the planet are never seen at all ) . xxmaj the environmental message offers no solutions , but paints one or two dire metaphors about what will happen to nature and man if something is n't done . xxmaj the logic also does n't hang together : the rest of the universe has \" given up \" on xxmaj earth , yet one space woman caught on xxmaj earth by mistake manages to effect some positive change by the conclusion of the movie . xxmaj what would a battalion of aliens deliberately sent here manage to achieve against pollution and waste ?,xxbos xxmaj after stopping by the movie store to find something to watch , we stumbled on this . xxmaj it looked appealing from the summary , at least , so we gave it a try . xxmaj and here 's the kicker : the first 20 minutes are interesting ! xxmaj it 's actually enjoyable ! xxmaj oh , wait , spoke too soon . \n",
       " \n",
       "  xxmaj somewhere in there , the movie took a disgusting turn into fundamental , right - wing xxmaj christian brain - washing . xxmaj not entirely sure what happens , but i think the screenplay writer found xxmaj god somewhere in there , finished writing this script , and had no time to edit it because he had a xxup kkk meeting to get to with his friends from the xxmaj xxunk xxmaj church and his hood was n't clean . \n",
       " \n",
       "  xxmaj can they put warnings on this ? i refuse to support this religious idiocy . xxmaj much like video games have rating systems , movies need some sort of symbol : maybe a small cross in the bottom corner to show us that a movie is going to take a turn for the worse . \n",
       " \n",
       "  xxmaj unless you share sentiments with whatever moron came up with this story , and will have your xxmaj bible open in your lap while you watch this and plan on how you 'll convert your neighbors , do n't waste your time . xxmaj it 's some of the worst junk that 's come out in a very long time , and the radical religious nuts do n't need anymore funding .,xxbos xxmaj the story line was very straight forward and easy to follow and contained a lot of no - brainer comedy to a point where it just got boring . xxmaj some of the audience seemed to find it funny but i like more intelligent humor . \n",
       " \n",
       "  xxmaj there were several known xxmaj swedish actors in the movie and their performance were decent considering the script . xxmaj lena xxmaj endre was good looking as always . \n",
       " \n",
       "  i do n't remember the original movie so i ca n't say if it 's better or worse . \n",
       " \n",
       "  xxmaj if you enjoy movies like xxmaj xxunk this movie might be worth taking a look at .,xxbos xxmaj no wonder this was released straight to xxup dvd here in xxmaj australia , no redeeming features what so ever . xxmaj the dialog was hokey , the acting , awful and the script sucked ! ! xxmaj whoever thought it would be a good idea to do a sequel or follow up to the far superior xxmaj john xxmaj badham film , xxmaj wargames from the 80s , well they must of been on something cause it was a bad idea ! ! xxmaj amanda xxmaj walsh was good in it as the eye candy / love interest , while xxmaj matt xxmaj xxunk was good as the other main lead- that is about it . i would not recommend xxmaj wargames : xxmaj the xxmaj dead xxmaj code to anyone , check out xxmaj hackers or the original xxmaj wargames film- both are better than this piece of crap ! !\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /storage/imdb;\n",
       "\n",
       "Valid: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj if you 're in the mood to laugh at a truly bad movie ( bad in the way only xxmaj ken xxmaj russell at his worst can be ) , you must try this one . xxmaj it succeeds in making you feel like you just landed in a xxunk porn - booth , and you can just about smell the urine on the floor . xxmaj kathleen xxmaj turner struts around in a blond wig , getting her kicks from \" pretending \" to be a two - bit hooker ( she really has a good solid job in the clothing industry and has been hurt so badly by men that this is the only way she can connect ) , and xxmaj tony xxmaj perkins plays a hysterical \" priest \" who is out to maybe murderer her ( yet another movie that ends with xxmaj tony xxmaj perkins in drag ) . xxmaj annie xxmaj potts shows up and is not allowed to provide an ounce of her usual wit , which is reason enough to hate this movie . xxmaj the kinky will love the sex scenes , so rent the unrated version in the xxup red box so you can see xxmaj turner give a cop a taste of his billy club ( i had to pause the xxup vcr until we stopped laughing ) .,xxbos i have n't seen this film in years , but the awful \" taste \" of xxmaj quaid 's performance still lingers on my tongue . xxmaj some have commented on how xxmaj quaid has xxmaj jerry xxmaj lee xxmaj lewis \" to a tee \" but the fact is he only appears to have the most extreme stage xxmaj jerry in mind . xxmaj nobody acts that way all the time , and the performance comes off as hopelessly clownish , reducing xxmaj lewis to a buffoonish caricature . xxmaj the nuances of a man 's life are lost in the rubble of sheer over - acting . \n",
       " \n",
       "  xxmaj the author of the book this is based on ( xxmaj nick xxmaj xxunk ) is a good writer , who has written several fine musical bios ( i particularly liked \" xxmaj dino \" on xxmaj dean xxmaj martin ) ; in the books xxmaj xxunk gives us a full human being , both separate from and involved in the \" biz . \" xxmaj quaid 's acting seems to imply that xxmaj jerry never acted like a human being . xxmaj if people were like this , no one would bother to hang around them . xxmaj as cartoons go , it is mildly amusing , but otherwise it is one of the most egregious , film - destroying performances i have had the \" honor \" of viewing . xxmaj terrible ...,xxbos \" xxmaj julia xxmaj xxunk ( xxmaj catherine xxmaj mary xxmaj stewart ) is working hard to become a doctor . xxmaj suddenly , xxmaj julia finds herself the guardian of her young niece , xxmaj amanda , after her parents are found murdered . xxmaj julia has a new neighbor , the mysterious xxmaj kevin xxmaj finney ( xxmaj rob xxmaj lowe ) . xxmaj her hectic life comes crashing down when it becomes apparent that the young xxmaj amanda holds the key to a terrible secret . xxmaj for she too is now the target of the killers . xxmaj julia must discover whether xxmaj kevin is friend or foe , and unlock the sinister mystery before the murderer strikes again , \" according to the xxup dvd sleeve synopsis . \n",
       " \n",
       "  xxmaj this thriller quietly falls apart at the seams , but it is interesting some of the time . xxmaj stalker xxmaj sean xxmaj devine 's background telephone scene ( outside the police station ) and xxmaj mr. xxmaj lowe 's violin ( restaurant ) are tensely played . xxmaj but , early on , it 's difficult not to miss the fact that one of the supposedly sophisticated killers purposely twists his foot in some red paint at the murder scene . xxmaj unfortunately , this is not an intricate plot point ; and , there are worse story stumbles afoot . xxmaj overall \" xxmaj dead xxmaj silent \" is not a bad way to spend some time , if there is nothing better on , or you 're into xxmaj lowe and his co - stars . \n",
       " \n",
       "  xxrep 4 * xxmaj dead xxmaj silent ( 1999 ) xxmaj roger xxmaj cardinal ~ xxmaj catherine xxmaj mary xxmaj stewart , xxmaj rob xxmaj lowe , xxmaj arlen xxmaj xxunk - xxmaj stewart , xxmaj larry xxmaj day,xxbos i watched about the first 30 - 40 minutes of this movie on television the other night and can agree that this is by far the worst of the series . xxmaj not any of it is funny , even xxmaj randy xxmaj quaid ca n't save this mess . xxmaj eric xxmaj idle was n't funny in xxup xxunk xxmaj euro . xxmaj vacation , and he 's even worse here . xxmaj the only funny scene is where they 're at the airport and some guy dressed as xxmaj santa walks by the camera yelling \" xxmaj did anybody lose this ? \" as he holds up a prosthetic leg ... \n",
       " \n",
       "  1 / 2 a star out of xxrep 4 *,xxbos xxmaj it looked cool from the movie sleeve , but after five minutes we were n't sure if it was a homosexual documentary of west side story without any female interest . xxmaj the film quality was poor , and there was hardly enough gang fighting action to sustain even the xxunk person 's interest for long enough to watch the entire film . xxmaj may god have mercy on the souls of both the actors and the filmmakers responsible for what i can only describe as my new one and only reason why i never will want to see ( or trust ) an xxmaj australian made film again . i have to write more so i will again say that the actors were so bad that i 'm positive i could make a better movie with fifteen dollars and a box of xxmaj xxunk . xxmaj please do n't see this movie for your own sake .\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /storage/imdb;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=LabelSmoothingCrossEntropy(), metrics=[<function accuracy at 0x7f5c1b29df28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/storage/imdb'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60000, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60000, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, pretrained=False,loss_func=LabelSmoothingCrossEntropy())\n",
    "learn.load_encoder('fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.callback_fns.append(partial(MixUp, alpha=0.4, stack_x=True, stack_y=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False),\n",
       " functools.partial(<class 'MixUp.MixUp'>, alpha=0.4, stack_x=True, stack_y=True)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.callback_fns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the model using gradual unfreezing (partially training the model from everything but the classification head frozen to the whole model trianing by unfreezing one layer at a time) and differential learning rate (deeper layer gets a lower learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.471571</td>\n",
       "      <td>0.342452</td>\n",
       "      <td>0.923880</td>\n",
       "      <td>02:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, lr, moms=(0.8,0.7), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.452473</td>\n",
       "      <td>0.306943</td>\n",
       "      <td>0.942640</td>\n",
       "      <td>02:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-2)\n",
    "lr /= 2\n",
    "learn.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.432925</td>\n",
       "      <td>0.297532</td>\n",
       "      <td>0.946680</td>\n",
       "      <td>03:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-3)\n",
    "lr /= 2\n",
    "learn.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj this movie is funny and painful at the same time . xxmaj the \" xxmaj cinemagic \" almost gave me a seizure . xxmaj despite what they imply , \" xxmaj cinemagic \" is not some innovative technical procedure . xxmaj it was \" developed \" as the result of an accident , and they used it because it disguised the fact that their \" monsters \" were so stupid - looking . i also do n't think it 's a coincidence that the writer is xxmaj sid \" xxmaj pink \" . \n",
       " \n",
       "  xxmaj this movie is good for a laugh , if you are really looking for a movie made in 9 days on 200,000 dollars . xxmaj it is entertaining ; at least i can say that about it . xxmaj the bat / rat / spider is the highlight .,xxbos xxup xxunk , a.k.a . xxup alien xxup visitor , is not what i expected . xxmaj this is a no - budget xxmaj australian film with no special effects other than speeded - up film and quick scene cuts . xxmaj the female alien ( who comes over immediately able to speak perfectly accented xxmaj australian ) can \" blip \" from place to place or time to time and alter her perception of the flow of time to match the \" faster \" humans . \n",
       " \n",
       "  xxmaj an elderly grandmother tells her two granddaughters about a story a wandering man told her 40 years before , when an unnamed \" xxmaj she \" came to the planet naked and completely disoriented , unable to recognize which star in the sky she came from ... xxmaj she meets a man alone camping in the xxmaj australian xxmaj outback , apparently bewildering him . xxmaj she is here by \" mistake \" , and gets angry when she is told she is on xxmaj earth . xxmaj the xxmaj earthlings are known as consummate xxunk of the environment and a metaphor for the most insulting thing imaginable to the rest of the universe : those who \" breathe the foul air \" but do nothing about it , sticking their heads under the sand like an ostrich . xxmaj in another amusing metaphor , xxmaj earthlings are \" frogs \" . \n",
       " \n",
       "  xxmaj from there , it is entirely a film about dialogue , as the perplexed man tries to understand xxmaj she 's peculiar psychology and viewpoints , even as xxmaj she calls him unintelligent and \" quaint \" . xxmaj the man begins to realize maybe it 's humans who are irrational and not thinking straight . xxmaj yet , while waiting to be \" beamed up \" back home , xxmaj she sees that this human is not entirely faulty in his thinking and even falls in love with him . \n",
       " \n",
       "  xxmaj the dialogue about perspectives is in spots interesting , but it is all layered with a heavy - handed environmental message and a low - budget feel ( there are only two main actors , who blip around various deserted scenes , and the evil despoiling humans on the planet are never seen at all ) . xxmaj the environmental message offers no solutions , but paints one or two dire metaphors about what will happen to nature and man if something is n't done . xxmaj the logic also does n't hang together : the rest of the universe has \" given up \" on xxmaj earth , yet one space woman caught on xxmaj earth by mistake manages to effect some positive change by the conclusion of the movie . xxmaj what would a battalion of aliens deliberately sent here manage to achieve against pollution and waste ?,xxbos xxmaj after stopping by the movie store to find something to watch , we stumbled on this . xxmaj it looked appealing from the summary , at least , so we gave it a try . xxmaj and here 's the kicker : the first 20 minutes are interesting ! xxmaj it 's actually enjoyable ! xxmaj oh , wait , spoke too soon . \n",
       " \n",
       "  xxmaj somewhere in there , the movie took a disgusting turn into fundamental , right - wing xxmaj christian brain - washing . xxmaj not entirely sure what happens , but i think the screenplay writer found xxmaj god somewhere in there , finished writing this script , and had no time to edit it because he had a xxup kkk meeting to get to with his friends from the xxmaj xxunk xxmaj church and his hood was n't clean . \n",
       " \n",
       "  xxmaj can they put warnings on this ? i refuse to support this religious idiocy . xxmaj much like video games have rating systems , movies need some sort of symbol : maybe a small cross in the bottom corner to show us that a movie is going to take a turn for the worse . \n",
       " \n",
       "  xxmaj unless you share sentiments with whatever moron came up with this story , and will have your xxmaj bible open in your lap while you watch this and plan on how you 'll convert your neighbors , do n't waste your time . xxmaj it 's some of the worst junk that 's come out in a very long time , and the radical religious nuts do n't need anymore funding .,xxbos xxmaj the story line was very straight forward and easy to follow and contained a lot of no - brainer comedy to a point where it just got boring . xxmaj some of the audience seemed to find it funny but i like more intelligent humor . \n",
       " \n",
       "  xxmaj there were several known xxmaj swedish actors in the movie and their performance were decent considering the script . xxmaj lena xxmaj endre was good looking as always . \n",
       " \n",
       "  i do n't remember the original movie so i ca n't say if it 's better or worse . \n",
       " \n",
       "  xxmaj if you enjoy movies like xxmaj xxunk this movie might be worth taking a look at .,xxbos xxmaj no wonder this was released straight to xxup dvd here in xxmaj australia , no redeeming features what so ever . xxmaj the dialog was hokey , the acting , awful and the script sucked ! ! xxmaj whoever thought it would be a good idea to do a sequel or follow up to the far superior xxmaj john xxmaj badham film , xxmaj wargames from the 80s , well they must of been on something cause it was a bad idea ! ! xxmaj amanda xxmaj walsh was good in it as the eye candy / love interest , while xxmaj matt xxmaj xxunk was good as the other main lead- that is about it . i would not recommend xxmaj wargames : xxmaj the xxmaj dead xxmaj code to anyone , check out xxmaj hackers or the original xxmaj wargames film- both are better than this piece of crap ! !\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /storage/imdb;\n",
       "\n",
       "Valid: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj if you 're in the mood to laugh at a truly bad movie ( bad in the way only xxmaj ken xxmaj russell at his worst can be ) , you must try this one . xxmaj it succeeds in making you feel like you just landed in a xxunk porn - booth , and you can just about smell the urine on the floor . xxmaj kathleen xxmaj turner struts around in a blond wig , getting her kicks from \" pretending \" to be a two - bit hooker ( she really has a good solid job in the clothing industry and has been hurt so badly by men that this is the only way she can connect ) , and xxmaj tony xxmaj perkins plays a hysterical \" priest \" who is out to maybe murderer her ( yet another movie that ends with xxmaj tony xxmaj perkins in drag ) . xxmaj annie xxmaj potts shows up and is not allowed to provide an ounce of her usual wit , which is reason enough to hate this movie . xxmaj the kinky will love the sex scenes , so rent the unrated version in the xxup red box so you can see xxmaj turner give a cop a taste of his billy club ( i had to pause the xxup vcr until we stopped laughing ) .,xxbos i have n't seen this film in years , but the awful \" taste \" of xxmaj quaid 's performance still lingers on my tongue . xxmaj some have commented on how xxmaj quaid has xxmaj jerry xxmaj lee xxmaj lewis \" to a tee \" but the fact is he only appears to have the most extreme stage xxmaj jerry in mind . xxmaj nobody acts that way all the time , and the performance comes off as hopelessly clownish , reducing xxmaj lewis to a buffoonish caricature . xxmaj the nuances of a man 's life are lost in the rubble of sheer over - acting . \n",
       " \n",
       "  xxmaj the author of the book this is based on ( xxmaj nick xxmaj xxunk ) is a good writer , who has written several fine musical bios ( i particularly liked \" xxmaj dino \" on xxmaj dean xxmaj martin ) ; in the books xxmaj xxunk gives us a full human being , both separate from and involved in the \" biz . \" xxmaj quaid 's acting seems to imply that xxmaj jerry never acted like a human being . xxmaj if people were like this , no one would bother to hang around them . xxmaj as cartoons go , it is mildly amusing , but otherwise it is one of the most egregious , film - destroying performances i have had the \" honor \" of viewing . xxmaj terrible ...,xxbos \" xxmaj julia xxmaj xxunk ( xxmaj catherine xxmaj mary xxmaj stewart ) is working hard to become a doctor . xxmaj suddenly , xxmaj julia finds herself the guardian of her young niece , xxmaj amanda , after her parents are found murdered . xxmaj julia has a new neighbor , the mysterious xxmaj kevin xxmaj finney ( xxmaj rob xxmaj lowe ) . xxmaj her hectic life comes crashing down when it becomes apparent that the young xxmaj amanda holds the key to a terrible secret . xxmaj for she too is now the target of the killers . xxmaj julia must discover whether xxmaj kevin is friend or foe , and unlock the sinister mystery before the murderer strikes again , \" according to the xxup dvd sleeve synopsis . \n",
       " \n",
       "  xxmaj this thriller quietly falls apart at the seams , but it is interesting some of the time . xxmaj stalker xxmaj sean xxmaj devine 's background telephone scene ( outside the police station ) and xxmaj mr. xxmaj lowe 's violin ( restaurant ) are tensely played . xxmaj but , early on , it 's difficult not to miss the fact that one of the supposedly sophisticated killers purposely twists his foot in some red paint at the murder scene . xxmaj unfortunately , this is not an intricate plot point ; and , there are worse story stumbles afoot . xxmaj overall \" xxmaj dead xxmaj silent \" is not a bad way to spend some time , if there is nothing better on , or you 're into xxmaj lowe and his co - stars . \n",
       " \n",
       "  xxrep 4 * xxmaj dead xxmaj silent ( 1999 ) xxmaj roger xxmaj cardinal ~ xxmaj catherine xxmaj mary xxmaj stewart , xxmaj rob xxmaj lowe , xxmaj arlen xxmaj xxunk - xxmaj stewart , xxmaj larry xxmaj day,xxbos i watched about the first 30 - 40 minutes of this movie on television the other night and can agree that this is by far the worst of the series . xxmaj not any of it is funny , even xxmaj randy xxmaj quaid ca n't save this mess . xxmaj eric xxmaj idle was n't funny in xxup xxunk xxmaj euro . xxmaj vacation , and he 's even worse here . xxmaj the only funny scene is where they 're at the airport and some guy dressed as xxmaj santa walks by the camera yelling \" xxmaj did anybody lose this ? \" as he holds up a prosthetic leg ... \n",
       " \n",
       "  1 / 2 a star out of xxrep 4 *,xxbos xxmaj it looked cool from the movie sleeve , but after five minutes we were n't sure if it was a homosexual documentary of west side story without any female interest . xxmaj the film quality was poor , and there was hardly enough gang fighting action to sustain even the xxunk person 's interest for long enough to watch the entire film . xxmaj may god have mercy on the souls of both the actors and the filmmakers responsible for what i can only describe as my new one and only reason why i never will want to see ( or trust ) an xxmaj australian made film again . i have to write more so i will again say that the actors were so bad that i 'm positive i could make a better movie with fifteen dollars and a box of xxmaj xxunk . xxmaj please do n't see this movie for your own sake .\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /storage/imdb;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=LabelSmoothingCrossEntropy(), metrics=[<function accuracy at 0x7f872904cd90>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/storage/imdb'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj this movie is funny and painful at the same time . xxmaj the \" xxmaj cinemagic \" almost gave me a seizure . xxmaj despite what they imply , \" xxmaj cinemagic \" is not some innovative technical procedure . xxmaj it was \" developed \" as the result of an accident , and they used it because it disguised the fact that their \" monsters \" were so stupid - looking . i also do n't think it 's a coincidence that the writer is xxmaj sid \" xxmaj pink \" . \n",
       " \n",
       "  xxmaj this movie is good for a laugh , if you are really looking for a movie made in 9 days on 200,000 dollars . xxmaj it is entertaining ; at least i can say that about it . xxmaj the bat / rat / spider is the highlight .,xxbos xxup xxunk , a.k.a . xxup alien xxup visitor , is not what i expected . xxmaj this is a no - budget xxmaj australian film with no special effects other than speeded - up film and quick scene cuts . xxmaj the female alien ( who comes over immediately able to speak perfectly accented xxmaj australian ) can \" blip \" from place to place or time to time and alter her perception of the flow of time to match the \" faster \" humans . \n",
       " \n",
       "  xxmaj an elderly grandmother tells her two granddaughters about a story a wandering man told her 40 years before , when an unnamed \" xxmaj she \" came to the planet naked and completely disoriented , unable to recognize which star in the sky she came from ... xxmaj she meets a man alone camping in the xxmaj australian xxmaj outback , apparently bewildering him . xxmaj she is here by \" mistake \" , and gets angry when she is told she is on xxmaj earth . xxmaj the xxmaj earthlings are known as consummate xxunk of the environment and a metaphor for the most insulting thing imaginable to the rest of the universe : those who \" breathe the foul air \" but do nothing about it , sticking their heads under the sand like an ostrich . xxmaj in another amusing metaphor , xxmaj earthlings are \" frogs \" . \n",
       " \n",
       "  xxmaj from there , it is entirely a film about dialogue , as the perplexed man tries to understand xxmaj she 's peculiar psychology and viewpoints , even as xxmaj she calls him unintelligent and \" quaint \" . xxmaj the man begins to realize maybe it 's humans who are irrational and not thinking straight . xxmaj yet , while waiting to be \" beamed up \" back home , xxmaj she sees that this human is not entirely faulty in his thinking and even falls in love with him . \n",
       " \n",
       "  xxmaj the dialogue about perspectives is in spots interesting , but it is all layered with a heavy - handed environmental message and a low - budget feel ( there are only two main actors , who blip around various deserted scenes , and the evil despoiling humans on the planet are never seen at all ) . xxmaj the environmental message offers no solutions , but paints one or two dire metaphors about what will happen to nature and man if something is n't done . xxmaj the logic also does n't hang together : the rest of the universe has \" given up \" on xxmaj earth , yet one space woman caught on xxmaj earth by mistake manages to effect some positive change by the conclusion of the movie . xxmaj what would a battalion of aliens deliberately sent here manage to achieve against pollution and waste ?,xxbos xxmaj after stopping by the movie store to find something to watch , we stumbled on this . xxmaj it looked appealing from the summary , at least , so we gave it a try . xxmaj and here 's the kicker : the first 20 minutes are interesting ! xxmaj it 's actually enjoyable ! xxmaj oh , wait , spoke too soon . \n",
       " \n",
       "  xxmaj somewhere in there , the movie took a disgusting turn into fundamental , right - wing xxmaj christian brain - washing . xxmaj not entirely sure what happens , but i think the screenplay writer found xxmaj god somewhere in there , finished writing this script , and had no time to edit it because he had a xxup kkk meeting to get to with his friends from the xxmaj xxunk xxmaj church and his hood was n't clean . \n",
       " \n",
       "  xxmaj can they put warnings on this ? i refuse to support this religious idiocy . xxmaj much like video games have rating systems , movies need some sort of symbol : maybe a small cross in the bottom corner to show us that a movie is going to take a turn for the worse . \n",
       " \n",
       "  xxmaj unless you share sentiments with whatever moron came up with this story , and will have your xxmaj bible open in your lap while you watch this and plan on how you 'll convert your neighbors , do n't waste your time . xxmaj it 's some of the worst junk that 's come out in a very long time , and the radical religious nuts do n't need anymore funding .,xxbos xxmaj the story line was very straight forward and easy to follow and contained a lot of no - brainer comedy to a point where it just got boring . xxmaj some of the audience seemed to find it funny but i like more intelligent humor . \n",
       " \n",
       "  xxmaj there were several known xxmaj swedish actors in the movie and their performance were decent considering the script . xxmaj lena xxmaj endre was good looking as always . \n",
       " \n",
       "  i do n't remember the original movie so i ca n't say if it 's better or worse . \n",
       " \n",
       "  xxmaj if you enjoy movies like xxmaj xxunk this movie might be worth taking a look at .,xxbos xxmaj no wonder this was released straight to xxup dvd here in xxmaj australia , no redeeming features what so ever . xxmaj the dialog was hokey , the acting , awful and the script sucked ! ! xxmaj whoever thought it would be a good idea to do a sequel or follow up to the far superior xxmaj john xxmaj badham film , xxmaj wargames from the 80s , well they must of been on something cause it was a bad idea ! ! xxmaj amanda xxmaj walsh was good in it as the eye candy / love interest , while xxmaj matt xxmaj xxunk was good as the other main lead- that is about it . i would not recommend xxmaj wargames : xxmaj the xxmaj dead xxmaj code to anyone , check out xxmaj hackers or the original xxmaj wargames film- both are better than this piece of crap ! !\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /storage/imdb;\n",
       "\n",
       "Valid: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj if you 're in the mood to laugh at a truly bad movie ( bad in the way only xxmaj ken xxmaj russell at his worst can be ) , you must try this one . xxmaj it succeeds in making you feel like you just landed in a xxunk porn - booth , and you can just about smell the urine on the floor . xxmaj kathleen xxmaj turner struts around in a blond wig , getting her kicks from \" pretending \" to be a two - bit hooker ( she really has a good solid job in the clothing industry and has been hurt so badly by men that this is the only way she can connect ) , and xxmaj tony xxmaj perkins plays a hysterical \" priest \" who is out to maybe murderer her ( yet another movie that ends with xxmaj tony xxmaj perkins in drag ) . xxmaj annie xxmaj potts shows up and is not allowed to provide an ounce of her usual wit , which is reason enough to hate this movie . xxmaj the kinky will love the sex scenes , so rent the unrated version in the xxup red box so you can see xxmaj turner give a cop a taste of his billy club ( i had to pause the xxup vcr until we stopped laughing ) .,xxbos i have n't seen this film in years , but the awful \" taste \" of xxmaj quaid 's performance still lingers on my tongue . xxmaj some have commented on how xxmaj quaid has xxmaj jerry xxmaj lee xxmaj lewis \" to a tee \" but the fact is he only appears to have the most extreme stage xxmaj jerry in mind . xxmaj nobody acts that way all the time , and the performance comes off as hopelessly clownish , reducing xxmaj lewis to a buffoonish caricature . xxmaj the nuances of a man 's life are lost in the rubble of sheer over - acting . \n",
       " \n",
       "  xxmaj the author of the book this is based on ( xxmaj nick xxmaj xxunk ) is a good writer , who has written several fine musical bios ( i particularly liked \" xxmaj dino \" on xxmaj dean xxmaj martin ) ; in the books xxmaj xxunk gives us a full human being , both separate from and involved in the \" biz . \" xxmaj quaid 's acting seems to imply that xxmaj jerry never acted like a human being . xxmaj if people were like this , no one would bother to hang around them . xxmaj as cartoons go , it is mildly amusing , but otherwise it is one of the most egregious , film - destroying performances i have had the \" honor \" of viewing . xxmaj terrible ...,xxbos \" xxmaj julia xxmaj xxunk ( xxmaj catherine xxmaj mary xxmaj stewart ) is working hard to become a doctor . xxmaj suddenly , xxmaj julia finds herself the guardian of her young niece , xxmaj amanda , after her parents are found murdered . xxmaj julia has a new neighbor , the mysterious xxmaj kevin xxmaj finney ( xxmaj rob xxmaj lowe ) . xxmaj her hectic life comes crashing down when it becomes apparent that the young xxmaj amanda holds the key to a terrible secret . xxmaj for she too is now the target of the killers . xxmaj julia must discover whether xxmaj kevin is friend or foe , and unlock the sinister mystery before the murderer strikes again , \" according to the xxup dvd sleeve synopsis . \n",
       " \n",
       "  xxmaj this thriller quietly falls apart at the seams , but it is interesting some of the time . xxmaj stalker xxmaj sean xxmaj devine 's background telephone scene ( outside the police station ) and xxmaj mr. xxmaj lowe 's violin ( restaurant ) are tensely played . xxmaj but , early on , it 's difficult not to miss the fact that one of the supposedly sophisticated killers purposely twists his foot in some red paint at the murder scene . xxmaj unfortunately , this is not an intricate plot point ; and , there are worse story stumbles afoot . xxmaj overall \" xxmaj dead xxmaj silent \" is not a bad way to spend some time , if there is nothing better on , or you 're into xxmaj lowe and his co - stars . \n",
       " \n",
       "  xxrep 4 * xxmaj dead xxmaj silent ( 1999 ) xxmaj roger xxmaj cardinal ~ xxmaj catherine xxmaj mary xxmaj stewart , xxmaj rob xxmaj lowe , xxmaj arlen xxmaj xxunk - xxmaj stewart , xxmaj larry xxmaj day,xxbos i watched about the first 30 - 40 minutes of this movie on television the other night and can agree that this is by far the worst of the series . xxmaj not any of it is funny , even xxmaj randy xxmaj quaid ca n't save this mess . xxmaj eric xxmaj idle was n't funny in xxup xxunk xxmaj euro . xxmaj vacation , and he 's even worse here . xxmaj the only funny scene is where they 're at the airport and some guy dressed as xxmaj santa walks by the camera yelling \" xxmaj did anybody lose this ? \" as he holds up a prosthetic leg ... \n",
       " \n",
       "  1 / 2 a star out of xxrep 4 *,xxbos xxmaj it looked cool from the movie sleeve , but after five minutes we were n't sure if it was a homosexual documentary of west side story without any female interest . xxmaj the film quality was poor , and there was hardly enough gang fighting action to sustain even the xxunk person 's interest for long enough to watch the entire film . xxmaj may god have mercy on the souls of both the actors and the filmmakers responsible for what i can only describe as my new one and only reason why i never will want to see ( or trust ) an xxmaj australian made film again . i have to write more so i will again say that the actors were so bad that i 'm positive i could make a better movie with fifteen dollars and a box of xxmaj xxunk . xxmaj please do n't see this movie for your own sake .\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /storage/imdb;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=LabelSmoothingCrossEntropy(), metrics=[<function accuracy at 0x7f872904cd90>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/storage/imdb'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60000, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60000, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('frozen_fwd_clas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.420039</td>\n",
       "      <td>0.292404</td>\n",
       "      <td>0.947120</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.407014</td>\n",
       "      <td>0.285854</td>\n",
       "      <td>0.949200</td>\n",
       "      <td>04:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.385696</td>\n",
       "      <td>0.285801</td>\n",
       "      <td>0.949600</td>\n",
       "      <td>04:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "lr /= 5\n",
    "learn.fit_one_cycle(3, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fwd_clas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same but backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do the same thing for the backward model, the only thigns to adjust are the names of the data object and the fine-tuned encoder we load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj this movie is funny and painful at the same time . xxmaj the \" xxmaj cinemagic \" almost gave me a seizure . xxmaj despite what they imply , \" xxmaj cinemagic \" is not some innovative technical procedure . xxmaj it was \" developed \" as the result of an accident , and they used it because it disguised the fact that their \" monsters \" were so stupid - looking . i also do n't think it 's a coincidence that the writer is xxmaj sid \" xxmaj pink \" . \n",
       " \n",
       "  xxmaj this movie is good for a laugh , if you are really looking for a movie made in 9 days on 200,000 dollars . xxmaj it is entertaining ; at least i can say that about it . xxmaj the bat / rat / spider is the highlight .,xxbos xxup xxunk , a.k.a . xxup alien xxup visitor , is not what i expected . xxmaj this is a no - budget xxmaj australian film with no special effects other than speeded - up film and quick scene cuts . xxmaj the female alien ( who comes over immediately able to speak perfectly accented xxmaj australian ) can \" blip \" from place to place or time to time and alter her perception of the flow of time to match the \" faster \" humans . \n",
       " \n",
       "  xxmaj an elderly grandmother tells her two granddaughters about a story a wandering man told her 40 years before , when an unnamed \" xxmaj she \" came to the planet naked and completely disoriented , unable to recognize which star in the sky she came from ... xxmaj she meets a man alone camping in the xxmaj australian xxmaj outback , apparently bewildering him . xxmaj she is here by \" mistake \" , and gets angry when she is told she is on xxmaj earth . xxmaj the xxmaj earthlings are known as consummate xxunk of the environment and a metaphor for the most insulting thing imaginable to the rest of the universe : those who \" breathe the foul air \" but do nothing about it , sticking their heads under the sand like an ostrich . xxmaj in another amusing metaphor , xxmaj earthlings are \" frogs \" . \n",
       " \n",
       "  xxmaj from there , it is entirely a film about dialogue , as the perplexed man tries to understand xxmaj she 's peculiar psychology and viewpoints , even as xxmaj she calls him unintelligent and \" quaint \" . xxmaj the man begins to realize maybe it 's humans who are irrational and not thinking straight . xxmaj yet , while waiting to be \" beamed up \" back home , xxmaj she sees that this human is not entirely faulty in his thinking and even falls in love with him . \n",
       " \n",
       "  xxmaj the dialogue about perspectives is in spots interesting , but it is all layered with a heavy - handed environmental message and a low - budget feel ( there are only two main actors , who blip around various deserted scenes , and the evil despoiling humans on the planet are never seen at all ) . xxmaj the environmental message offers no solutions , but paints one or two dire metaphors about what will happen to nature and man if something is n't done . xxmaj the logic also does n't hang together : the rest of the universe has \" given up \" on xxmaj earth , yet one space woman caught on xxmaj earth by mistake manages to effect some positive change by the conclusion of the movie . xxmaj what would a battalion of aliens deliberately sent here manage to achieve against pollution and waste ?,xxbos xxmaj after stopping by the movie store to find something to watch , we stumbled on this . xxmaj it looked appealing from the summary , at least , so we gave it a try . xxmaj and here 's the kicker : the first 20 minutes are interesting ! xxmaj it 's actually enjoyable ! xxmaj oh , wait , spoke too soon . \n",
       " \n",
       "  xxmaj somewhere in there , the movie took a disgusting turn into fundamental , right - wing xxmaj christian brain - washing . xxmaj not entirely sure what happens , but i think the screenplay writer found xxmaj god somewhere in there , finished writing this script , and had no time to edit it because he had a xxup kkk meeting to get to with his friends from the xxmaj xxunk xxmaj church and his hood was n't clean . \n",
       " \n",
       "  xxmaj can they put warnings on this ? i refuse to support this religious idiocy . xxmaj much like video games have rating systems , movies need some sort of symbol : maybe a small cross in the bottom corner to show us that a movie is going to take a turn for the worse . \n",
       " \n",
       "  xxmaj unless you share sentiments with whatever moron came up with this story , and will have your xxmaj bible open in your lap while you watch this and plan on how you 'll convert your neighbors , do n't waste your time . xxmaj it 's some of the worst junk that 's come out in a very long time , and the radical religious nuts do n't need anymore funding .,xxbos xxmaj the story line was very straight forward and easy to follow and contained a lot of no - brainer comedy to a point where it just got boring . xxmaj some of the audience seemed to find it funny but i like more intelligent humor . \n",
       " \n",
       "  xxmaj there were several known xxmaj swedish actors in the movie and their performance were decent considering the script . xxmaj lena xxmaj endre was good looking as always . \n",
       " \n",
       "  i do n't remember the original movie so i ca n't say if it 's better or worse . \n",
       " \n",
       "  xxmaj if you enjoy movies like xxmaj xxunk this movie might be worth taking a look at .,xxbos xxmaj no wonder this was released straight to xxup dvd here in xxmaj australia , no redeeming features what so ever . xxmaj the dialog was hokey , the acting , awful and the script sucked ! ! xxmaj whoever thought it would be a good idea to do a sequel or follow up to the far superior xxmaj john xxmaj badham film , xxmaj wargames from the 80s , well they must of been on something cause it was a bad idea ! ! xxmaj amanda xxmaj walsh was good in it as the eye candy / love interest , while xxmaj matt xxmaj xxunk was good as the other main lead- that is about it . i would not recommend xxmaj wargames : xxmaj the xxmaj dead xxmaj code to anyone , check out xxmaj hackers or the original xxmaj wargames film- both are better than this piece of crap ! !\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /storage/imdb;\n",
       "\n",
       "Valid: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj if you 're in the mood to laugh at a truly bad movie ( bad in the way only xxmaj ken xxmaj russell at his worst can be ) , you must try this one . xxmaj it succeeds in making you feel like you just landed in a xxunk porn - booth , and you can just about smell the urine on the floor . xxmaj kathleen xxmaj turner struts around in a blond wig , getting her kicks from \" pretending \" to be a two - bit hooker ( she really has a good solid job in the clothing industry and has been hurt so badly by men that this is the only way she can connect ) , and xxmaj tony xxmaj perkins plays a hysterical \" priest \" who is out to maybe murderer her ( yet another movie that ends with xxmaj tony xxmaj perkins in drag ) . xxmaj annie xxmaj potts shows up and is not allowed to provide an ounce of her usual wit , which is reason enough to hate this movie . xxmaj the kinky will love the sex scenes , so rent the unrated version in the xxup red box so you can see xxmaj turner give a cop a taste of his billy club ( i had to pause the xxup vcr until we stopped laughing ) .,xxbos i have n't seen this film in years , but the awful \" taste \" of xxmaj quaid 's performance still lingers on my tongue . xxmaj some have commented on how xxmaj quaid has xxmaj jerry xxmaj lee xxmaj lewis \" to a tee \" but the fact is he only appears to have the most extreme stage xxmaj jerry in mind . xxmaj nobody acts that way all the time , and the performance comes off as hopelessly clownish , reducing xxmaj lewis to a buffoonish caricature . xxmaj the nuances of a man 's life are lost in the rubble of sheer over - acting . \n",
       " \n",
       "  xxmaj the author of the book this is based on ( xxmaj nick xxmaj xxunk ) is a good writer , who has written several fine musical bios ( i particularly liked \" xxmaj dino \" on xxmaj dean xxmaj martin ) ; in the books xxmaj xxunk gives us a full human being , both separate from and involved in the \" biz . \" xxmaj quaid 's acting seems to imply that xxmaj jerry never acted like a human being . xxmaj if people were like this , no one would bother to hang around them . xxmaj as cartoons go , it is mildly amusing , but otherwise it is one of the most egregious , film - destroying performances i have had the \" honor \" of viewing . xxmaj terrible ...,xxbos \" xxmaj julia xxmaj xxunk ( xxmaj catherine xxmaj mary xxmaj stewart ) is working hard to become a doctor . xxmaj suddenly , xxmaj julia finds herself the guardian of her young niece , xxmaj amanda , after her parents are found murdered . xxmaj julia has a new neighbor , the mysterious xxmaj kevin xxmaj finney ( xxmaj rob xxmaj lowe ) . xxmaj her hectic life comes crashing down when it becomes apparent that the young xxmaj amanda holds the key to a terrible secret . xxmaj for she too is now the target of the killers . xxmaj julia must discover whether xxmaj kevin is friend or foe , and unlock the sinister mystery before the murderer strikes again , \" according to the xxup dvd sleeve synopsis . \n",
       " \n",
       "  xxmaj this thriller quietly falls apart at the seams , but it is interesting some of the time . xxmaj stalker xxmaj sean xxmaj devine 's background telephone scene ( outside the police station ) and xxmaj mr. xxmaj lowe 's violin ( restaurant ) are tensely played . xxmaj but , early on , it 's difficult not to miss the fact that one of the supposedly sophisticated killers purposely twists his foot in some red paint at the murder scene . xxmaj unfortunately , this is not an intricate plot point ; and , there are worse story stumbles afoot . xxmaj overall \" xxmaj dead xxmaj silent \" is not a bad way to spend some time , if there is nothing better on , or you 're into xxmaj lowe and his co - stars . \n",
       " \n",
       "  xxrep 4 * xxmaj dead xxmaj silent ( 1999 ) xxmaj roger xxmaj cardinal ~ xxmaj catherine xxmaj mary xxmaj stewart , xxmaj rob xxmaj lowe , xxmaj arlen xxmaj xxunk - xxmaj stewart , xxmaj larry xxmaj day,xxbos i watched about the first 30 - 40 minutes of this movie on television the other night and can agree that this is by far the worst of the series . xxmaj not any of it is funny , even xxmaj randy xxmaj quaid ca n't save this mess . xxmaj eric xxmaj idle was n't funny in xxup xxunk xxmaj euro . xxmaj vacation , and he 's even worse here . xxmaj the only funny scene is where they 're at the airport and some guy dressed as xxmaj santa walks by the camera yelling \" xxmaj did anybody lose this ? \" as he holds up a prosthetic leg ... \n",
       " \n",
       "  1 / 2 a star out of xxrep 4 *,xxbos xxmaj it looked cool from the movie sleeve , but after five minutes we were n't sure if it was a homosexual documentary of west side story without any female interest . xxmaj the film quality was poor , and there was hardly enough gang fighting action to sustain even the xxunk person 's interest for long enough to watch the entire film . xxmaj may god have mercy on the souls of both the actors and the filmmakers responsible for what i can only describe as my new one and only reason why i never will want to see ( or trust ) an xxmaj australian made film again . i have to write more so i will again say that the actors were so bad that i 'm positive i could make a better movie with fifteen dollars and a box of xxmaj xxunk . xxmaj please do n't see this movie for your own sake .\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /storage/imdb;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=LabelSmoothingCrossEntropy(), metrics=[<function accuracy at 0x7f5c1b29df28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/storage/imdb'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj this movie is funny and painful at the same time . xxmaj the \" xxmaj cinemagic \" almost gave me a seizure . xxmaj despite what they imply , \" xxmaj cinemagic \" is not some innovative technical procedure . xxmaj it was \" developed \" as the result of an accident , and they used it because it disguised the fact that their \" monsters \" were so stupid - looking . i also do n't think it 's a coincidence that the writer is xxmaj sid \" xxmaj pink \" . \n",
       " \n",
       "  xxmaj this movie is good for a laugh , if you are really looking for a movie made in 9 days on 200,000 dollars . xxmaj it is entertaining ; at least i can say that about it . xxmaj the bat / rat / spider is the highlight .,xxbos xxup xxunk , a.k.a . xxup alien xxup visitor , is not what i expected . xxmaj this is a no - budget xxmaj australian film with no special effects other than speeded - up film and quick scene cuts . xxmaj the female alien ( who comes over immediately able to speak perfectly accented xxmaj australian ) can \" blip \" from place to place or time to time and alter her perception of the flow of time to match the \" faster \" humans . \n",
       " \n",
       "  xxmaj an elderly grandmother tells her two granddaughters about a story a wandering man told her 40 years before , when an unnamed \" xxmaj she \" came to the planet naked and completely disoriented , unable to recognize which star in the sky she came from ... xxmaj she meets a man alone camping in the xxmaj australian xxmaj outback , apparently bewildering him . xxmaj she is here by \" mistake \" , and gets angry when she is told she is on xxmaj earth . xxmaj the xxmaj earthlings are known as consummate xxunk of the environment and a metaphor for the most insulting thing imaginable to the rest of the universe : those who \" breathe the foul air \" but do nothing about it , sticking their heads under the sand like an ostrich . xxmaj in another amusing metaphor , xxmaj earthlings are \" frogs \" . \n",
       " \n",
       "  xxmaj from there , it is entirely a film about dialogue , as the perplexed man tries to understand xxmaj she 's peculiar psychology and viewpoints , even as xxmaj she calls him unintelligent and \" quaint \" . xxmaj the man begins to realize maybe it 's humans who are irrational and not thinking straight . xxmaj yet , while waiting to be \" beamed up \" back home , xxmaj she sees that this human is not entirely faulty in his thinking and even falls in love with him . \n",
       " \n",
       "  xxmaj the dialogue about perspectives is in spots interesting , but it is all layered with a heavy - handed environmental message and a low - budget feel ( there are only two main actors , who blip around various deserted scenes , and the evil despoiling humans on the planet are never seen at all ) . xxmaj the environmental message offers no solutions , but paints one or two dire metaphors about what will happen to nature and man if something is n't done . xxmaj the logic also does n't hang together : the rest of the universe has \" given up \" on xxmaj earth , yet one space woman caught on xxmaj earth by mistake manages to effect some positive change by the conclusion of the movie . xxmaj what would a battalion of aliens deliberately sent here manage to achieve against pollution and waste ?,xxbos xxmaj after stopping by the movie store to find something to watch , we stumbled on this . xxmaj it looked appealing from the summary , at least , so we gave it a try . xxmaj and here 's the kicker : the first 20 minutes are interesting ! xxmaj it 's actually enjoyable ! xxmaj oh , wait , spoke too soon . \n",
       " \n",
       "  xxmaj somewhere in there , the movie took a disgusting turn into fundamental , right - wing xxmaj christian brain - washing . xxmaj not entirely sure what happens , but i think the screenplay writer found xxmaj god somewhere in there , finished writing this script , and had no time to edit it because he had a xxup kkk meeting to get to with his friends from the xxmaj xxunk xxmaj church and his hood was n't clean . \n",
       " \n",
       "  xxmaj can they put warnings on this ? i refuse to support this religious idiocy . xxmaj much like video games have rating systems , movies need some sort of symbol : maybe a small cross in the bottom corner to show us that a movie is going to take a turn for the worse . \n",
       " \n",
       "  xxmaj unless you share sentiments with whatever moron came up with this story , and will have your xxmaj bible open in your lap while you watch this and plan on how you 'll convert your neighbors , do n't waste your time . xxmaj it 's some of the worst junk that 's come out in a very long time , and the radical religious nuts do n't need anymore funding .,xxbos xxmaj the story line was very straight forward and easy to follow and contained a lot of no - brainer comedy to a point where it just got boring . xxmaj some of the audience seemed to find it funny but i like more intelligent humor . \n",
       " \n",
       "  xxmaj there were several known xxmaj swedish actors in the movie and their performance were decent considering the script . xxmaj lena xxmaj endre was good looking as always . \n",
       " \n",
       "  i do n't remember the original movie so i ca n't say if it 's better or worse . \n",
       " \n",
       "  xxmaj if you enjoy movies like xxmaj xxunk this movie might be worth taking a look at .,xxbos xxmaj no wonder this was released straight to xxup dvd here in xxmaj australia , no redeeming features what so ever . xxmaj the dialog was hokey , the acting , awful and the script sucked ! ! xxmaj whoever thought it would be a good idea to do a sequel or follow up to the far superior xxmaj john xxmaj badham film , xxmaj wargames from the 80s , well they must of been on something cause it was a bad idea ! ! xxmaj amanda xxmaj walsh was good in it as the eye candy / love interest , while xxmaj matt xxmaj xxunk was good as the other main lead- that is about it . i would not recommend xxmaj wargames : xxmaj the xxmaj dead xxmaj code to anyone , check out xxmaj hackers or the original xxmaj wargames film- both are better than this piece of crap ! !\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /storage/imdb;\n",
       "\n",
       "Valid: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj if you 're in the mood to laugh at a truly bad movie ( bad in the way only xxmaj ken xxmaj russell at his worst can be ) , you must try this one . xxmaj it succeeds in making you feel like you just landed in a xxunk porn - booth , and you can just about smell the urine on the floor . xxmaj kathleen xxmaj turner struts around in a blond wig , getting her kicks from \" pretending \" to be a two - bit hooker ( she really has a good solid job in the clothing industry and has been hurt so badly by men that this is the only way she can connect ) , and xxmaj tony xxmaj perkins plays a hysterical \" priest \" who is out to maybe murderer her ( yet another movie that ends with xxmaj tony xxmaj perkins in drag ) . xxmaj annie xxmaj potts shows up and is not allowed to provide an ounce of her usual wit , which is reason enough to hate this movie . xxmaj the kinky will love the sex scenes , so rent the unrated version in the xxup red box so you can see xxmaj turner give a cop a taste of his billy club ( i had to pause the xxup vcr until we stopped laughing ) .,xxbos i have n't seen this film in years , but the awful \" taste \" of xxmaj quaid 's performance still lingers on my tongue . xxmaj some have commented on how xxmaj quaid has xxmaj jerry xxmaj lee xxmaj lewis \" to a tee \" but the fact is he only appears to have the most extreme stage xxmaj jerry in mind . xxmaj nobody acts that way all the time , and the performance comes off as hopelessly clownish , reducing xxmaj lewis to a buffoonish caricature . xxmaj the nuances of a man 's life are lost in the rubble of sheer over - acting . \n",
       " \n",
       "  xxmaj the author of the book this is based on ( xxmaj nick xxmaj xxunk ) is a good writer , who has written several fine musical bios ( i particularly liked \" xxmaj dino \" on xxmaj dean xxmaj martin ) ; in the books xxmaj xxunk gives us a full human being , both separate from and involved in the \" biz . \" xxmaj quaid 's acting seems to imply that xxmaj jerry never acted like a human being . xxmaj if people were like this , no one would bother to hang around them . xxmaj as cartoons go , it is mildly amusing , but otherwise it is one of the most egregious , film - destroying performances i have had the \" honor \" of viewing . xxmaj terrible ...,xxbos \" xxmaj julia xxmaj xxunk ( xxmaj catherine xxmaj mary xxmaj stewart ) is working hard to become a doctor . xxmaj suddenly , xxmaj julia finds herself the guardian of her young niece , xxmaj amanda , after her parents are found murdered . xxmaj julia has a new neighbor , the mysterious xxmaj kevin xxmaj finney ( xxmaj rob xxmaj lowe ) . xxmaj her hectic life comes crashing down when it becomes apparent that the young xxmaj amanda holds the key to a terrible secret . xxmaj for she too is now the target of the killers . xxmaj julia must discover whether xxmaj kevin is friend or foe , and unlock the sinister mystery before the murderer strikes again , \" according to the xxup dvd sleeve synopsis . \n",
       " \n",
       "  xxmaj this thriller quietly falls apart at the seams , but it is interesting some of the time . xxmaj stalker xxmaj sean xxmaj devine 's background telephone scene ( outside the police station ) and xxmaj mr. xxmaj lowe 's violin ( restaurant ) are tensely played . xxmaj but , early on , it 's difficult not to miss the fact that one of the supposedly sophisticated killers purposely twists his foot in some red paint at the murder scene . xxmaj unfortunately , this is not an intricate plot point ; and , there are worse story stumbles afoot . xxmaj overall \" xxmaj dead xxmaj silent \" is not a bad way to spend some time , if there is nothing better on , or you 're into xxmaj lowe and his co - stars . \n",
       " \n",
       "  xxrep 4 * xxmaj dead xxmaj silent ( 1999 ) xxmaj roger xxmaj cardinal ~ xxmaj catherine xxmaj mary xxmaj stewart , xxmaj rob xxmaj lowe , xxmaj arlen xxmaj xxunk - xxmaj stewart , xxmaj larry xxmaj day,xxbos i watched about the first 30 - 40 minutes of this movie on television the other night and can agree that this is by far the worst of the series . xxmaj not any of it is funny , even xxmaj randy xxmaj quaid ca n't save this mess . xxmaj eric xxmaj idle was n't funny in xxup xxunk xxmaj euro . xxmaj vacation , and he 's even worse here . xxmaj the only funny scene is where they 're at the airport and some guy dressed as xxmaj santa walks by the camera yelling \" xxmaj did anybody lose this ? \" as he holds up a prosthetic leg ... \n",
       " \n",
       "  1 / 2 a star out of xxrep 4 *,xxbos xxmaj it looked cool from the movie sleeve , but after five minutes we were n't sure if it was a homosexual documentary of west side story without any female interest . xxmaj the film quality was poor , and there was hardly enough gang fighting action to sustain even the xxunk person 's interest for long enough to watch the entire film . xxmaj may god have mercy on the souls of both the actors and the filmmakers responsible for what i can only describe as my new one and only reason why i never will want to see ( or trust ) an xxmaj australian made film again . i have to write more so i will again say that the actors were so bad that i 'm positive i could make a better movie with fifteen dollars and a box of xxmaj xxunk . xxmaj please do n't see this movie for your own sake .\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /storage/imdb;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=LabelSmoothingCrossEntropy(), metrics=[<function accuracy at 0x7f5c1b29df28>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/storage/imdb'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60000, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60000, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_bwd = text_classifier_learner(data_clas_bwd, AWD_LSTM, drop_mult=0.5, pretrained=False,loss_func=LabelSmoothingCrossEntropy())\n",
    "learn_bwd.load_encoder('bw_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_bwd.callback_fns.append(partial(MixUp, alpha=0.4, stack_x=True, stack_y=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.491216</td>\n",
       "      <td>0.351892</td>\n",
       "      <td>0.909840</td>\n",
       "      <td>02:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_bwd.fit_one_cycle(1, lr, moms=(0.8,0.7), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.452208</td>\n",
       "      <td>0.314326</td>\n",
       "      <td>0.937360</td>\n",
       "      <td>02:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_bwd.freeze_to(-2)\n",
    "lr /= 2\n",
    "learn_bwd.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.439656</td>\n",
       "      <td>0.301123</td>\n",
       "      <td>0.943240</td>\n",
       "      <td>03:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_bwd.freeze_to(-3)\n",
    "lr /= 2\n",
    "learn_bwd.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.422166</td>\n",
       "      <td>0.299733</td>\n",
       "      <td>0.942720</td>\n",
       "      <td>04:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.410963</td>\n",
       "      <td>0.298268</td>\n",
       "      <td>0.944160</td>\n",
       "      <td>04:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.393983</td>\n",
       "      <td>0.295265</td>\n",
       "      <td>0.947120</td>\n",
       "      <td>04:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_bwd.unfreeze()\n",
    "lr /= 4\n",
    "learn_bwd.fit_one_cycle(3,slice(lr/(2.6**4),lr), moms=(0.8,0.7), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_bwd.save('bwd_clas20200531')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling the two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final results, we'll take the average of the predictions of the forward and the backward models. SInce the samples are sorted by text lengths for batching, we pass the argument `ordered=True` to get the predictions in the order of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_fwd,lbl_fwd = learn.get_preds(ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_bwd,lbl_bwd = learn_bwd.get_preds(ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = (pred_fwd+pred_bwd)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9536)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(final_pred, lbl_fwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get the 95.4% accuracy reported in the paper!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
